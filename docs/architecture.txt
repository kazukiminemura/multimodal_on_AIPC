## Repository Structure
```
multimodal_on_AIPC/
├── download_models.py        # CLI wrapper to force model downloads into data/models
├── docs/                     # Requirements & architecture documentation
├── requirements.txt          # Python dependencies
├── src/app/
│   ├── __init__.py
│   ├── config.py             # Environment-driven settings (model repos, cache dir, toggles)
│   ├── download_models.py    # Async entry point used by the CLI wrapper
│   ├── image_client.py       # Stable Diffusion INT8 OpenVINO client (mock + HTTP)
│   ├── llm_client.py         # DeepSeek OpenVINO client (mock + HTTP)
│   ├── main.py               # FastAPI app factory, startup hook, static assets
│   ├── model_downloader.py   # Hugging Face snapshot helper with force download support
│   ├── orchestrator.py       # Conversation store, LLM + diffusion coordination
│   ├── prompt_manager.py     # System/developer prompt assembly for DeepSeek
│   ├── routes.py             # `/chat` and `/health` endpoints
│   └── schemas.py            # Pydantic models shared across components
└── static/                   # index.html, chat.js, style.css, mock-image.svg
```

## Initialization Flow
1. `uvicorn app.main:app --app-dir src` starts FastAPI.
2. `app/main.py` mounts the static UI, registers routes, and calls `ensure_models_async()` on startup.
3. `model_downloader.py` checks whether Hugging Face snapshots for
   - `OpenVINO/DeepSeek-R1-Distill-Qwen-1.5B-int4-ov`
   - `OpenVINO/stable-diffusion-v1-5-int8-ov`
   exist under `data/models/`; if not, it downloads them (unless `USE_MOCKS=true` and not forced).
4. Requests are handled by the orchestrator, which constructs prompts, invokes the LLM, and forwards image prompts to the diffusion client when required.

## Runtime Components
- **PromptManager** – Prepends system/developer instructions and session history to LLM prompts.
- **DeepSeekClient** – Talks to the DeepSeek OpenVINO inference endpoint (OpenAI-compatible JSON); includes a mock path for local testing.
- **StableDiffusionClient** – Calls a Stable Diffusion v1.5 INT8 OpenVINO service; mock path returns local placeholder assets.
- **ConversationStore** – Keeps a bounded deque per user (default 10 exchanges) for context retention.
- **ChatOrchestrator** – Central coordinator that merges history, calls the LLM, triggers diffusion, and assembles the final `ChatResponse`.
- **ModelDownloader** – Wraps `huggingface_hub.snapshot_download` to populate `data/models/deepseek/` and `data/models/stable-diffusion/`; also used by the standalone `download_models.py` script for pre-population.
- **Static UI** – A single-page HTML/JS client that consumes `/chat`, stores session history in `sessionStorage`, and displays generated images.

## Request Lifecycle
1. Browser or client POSTs `{ user_id, message }` to `/chat`.
2. FastAPI validates against `ChatRequest`, fetches history from `ConversationStore`, and builds LLM messages.
3. `DeepSeekClient.generate()` returns `assistant_response` plus optional `image_prompt`.
4. If an image prompt exists, `StableDiffusionClient.generate()` returns job metadata and URLs; otherwise the orchestrator skips this step.
5. `ChatResponse` is returned with text, flags, optional job ID, and URLs. The UI immediately renders text and thumbnails (mock or real).

## Configuration Surface
Environment variables in `config.py` control:
- `DEEPSEEK_ENDPOINT`, `STABLE_DIFFUSION_ENDPOINT`, and corresponding API keys.
- `DEEPSEEK_REPO_ID` (defaults to `OpenVINO/DeepSeek-R1-Distill-Qwen-1.5B-int4-ov`), `STABLE_DIFFUSION_REPO_ID` (defaults to `OpenVINO/stable-diffusion-v1-5-int8-ov`), and optional revisions.
- `MODELS_CACHE_DIR` (defaults to `data/models`), `AUTO_DOWNLOAD_MODELS`, `USE_MOCKS`, `REQUEST_TIMEOUT`, `CONVERSATION_HISTORY_LIMIT`.
- `HUGGINGFACE_TOKEN` for gated repositories.

`download_models.py` forcibly downloads models (ignoring `USE_MOCKS`) so deployments can stage weights before first run.

## Deployment Considerations
- Package the FastAPI app via Uvicorn or Gunicorn and front it with an API gateway for TLS, auth, and rate limiting.
- Host DeepSeek and Stable Diffusion services on hardware with OpenVINO support; scale them independently.
- Add Redis or another cache for session data if multiple orchestrator instances run behind a load balancer.
- Integrate monitoring (Prometheus, OpenTelemetry) to track request latency, GPU/NPU utilization, download status, and failures.
- Enable NSFW filtering and prompt sanitation before invoking diffusion in production environments.

## Testing & Validation
- Unit-test prompt assembly, schema validation, and conversation-store behavior.
- Mock the LLM/diffusion clients to verify orchestrator branching.
- Run integration smoke tests hitting `/chat` for text-only and text+image cases.
- Exercise `python download_models.py` in CI to verify huggingface-hub integration (can be skipped with mocks).
