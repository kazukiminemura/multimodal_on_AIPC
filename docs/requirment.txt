# Multimodal Chatbot Requirements

## 1. Goals
- Deliver an offline-friendly chatbot that couples **OpenVINO/DeepSeek-R1-Distill-Qwen-1.5B-int4-ov** for conversation with **OpenVINO/stable-diffusion-v1-5-int8-ov** for image synthesis.
- Let a single prompt trigger either text-only replies or text plus generated imagery via one API.
- Provide a lightweight UI and API that run entirely on a user-controlled workstation (Intel AI PC or compatible hardware).

## 2. Target Users
- Makers and developers experimenting with multimodal AI pipelines on personal hardware.
- Designers and storytellers who need quick visual drafts to accompany generated text.
- Privacy-conscious users who prefer not to send prompts or outputs to cloud services.

## 3. Functional Requirements
### 3.1 Chat Interface
- Expose `POST /chat` that accepts `{ "user_id": "...", "message": "..." }`.
- Maintain per-user conversational context for up to `CONVERSATION_HISTORY_LIMIT` turns.
- Return structured JSON with `assistant_response` and `image_urls` / `image_job_id` when imagery is produced.

### 3.2 Image Generation
- Detect an `image_prompt` block in the LLM response; if present, invoke the Stable Diffusion INT8 runtime.
- Support prompt parameters coming from the LLM output (negative prompt, steps, cfg scale, seed).
- Store or expose generated image locations so the UI can display thumbnails immediately.

### 3.3 Model Management
- On startup, ensure both OpenVINO model weights exist under `data/models`; auto-download them from Hugging Face when permitted.
- Provide a script (`python download_models.py`) that force-downloads models regardless of mock flags.
- Allow switching between mock mode (`USE_MOCKS=true`) and real inference for development.

### 3.4 Configuration & Operations
- Read endpoints, API keys, timeouts, cache directories, and feature flags from environment variables or `.env`.
- Offer `GET /health` reporting service status, mock usage, and whether models are cached.
- Emit structured logs for chat requests, download events, and errors to aid observability.

## 4. Non-functional Requirements
- Aim for <2 s P95 latency for text-only prompts on recommended hardware.
- Complete combined text + image requests within 20 s P95, including queue time.
- Support at least 200 concurrent chat sessions without non-graceful degradation.
- Operate offline once models are cached; no outbound network calls during inference.
- Enforce authentication and per-user rate limiting to prevent misuse (future enhancement).

## 5. Technical Constraints
- Backend: Python 3.10â€“3.12 with FastAPI, Uvicorn, httpx, Pydantic, huggingface-hub.
- LLM runtime: OpenVINO execution of DeepSeek-R1-Distill-Qwen-1.5B-int4-ov via local inference server.
- Diffusion runtime: OpenVINO deployment of stable-diffusion-v1-5-int8-ov or API-compatible service.
- Persistence (optional): Redis for session cache, object storage for generated assets if long-term retention is needed.

## 6. Setup Flow
1. Clone the repository and create a Python virtual environment.
2. Install dependencies with `pip install -r requirements.txt`.
3. Run `python download_models.py` (requires Hugging Face token when necessary) or set `USE_MOCKS=true` for dry runs.
4. Launch `uvicorn app.main:app --app-dir src --reload` and open the UI at `http://127.0.0.1:8000/`.

## 7. Future Enhancements
- Persist conversation history in a database with search and tagging.
- Add WebSocket streaming for partial text responses and real-time image job updates.
- Integrate NSFW/safety filters between the LLM output and diffusion calls.
- Provide template libraries, gallery management, and asset export from the UI.
